{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProtoryNet",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbYnh1SGtqzaA3AIZ1SIcK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dathong/ProtoryNet/blob/master/ProtoryNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTJgmO7pjyUQ",
        "colab_type": "code",
        "outputId": "4f44b053-1960-4603-fcf0-443d8a0baf14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "#import\n",
        "!pip install https://github.com/scikit-learn-contrib/scikit-learn-extra/archive/master.zip\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import nltk, re, time\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from collections import namedtuple\n",
        "import os\n",
        "import pickle\n",
        "\n"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/scikit-learn-contrib/scikit-learn-extra/archive/master.zip\n",
            "  Using cached https://github.com/scikit-learn-contrib/scikit-learn-extra/archive/master.zip\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): scikit-learn-extra==0.1.0b2 from https://github.com/scikit-learn-contrib/scikit-learn-extra/archive/master.zip in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra==0.1.0b2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra==0.1.0b2) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra==0.1.0b2) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->scikit-learn-extra==0.1.0b2) (0.15.1)\n",
            "Building wheels for collected packages: scikit-learn-extra\n",
            "  Building wheel for scikit-learn-extra (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-learn-extra: filename=scikit_learn_extra-0.1.0b2-cp36-cp36m-linux_x86_64.whl size=342229 sha256=6b74d77b49daff5b28eaf8b4f05bd44dd3d907a6511097738f9f02bcc3c2e4a5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w4462d7l/wheels/d3/a5/a8/411bc2d0939f2cc9d17f34f0d3457043c68e14a98b91fd8301\n",
            "Successfully built scikit-learn-extra\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4WH_aB7qSwh",
        "colab_type": "code",
        "outputId": "6f527956-3790-4be7-e456-085e0e1637a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(tf.__version__)\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16-LvWrhkZYi",
        "colab_type": "code",
        "outputId": "447b04ec-9aa3-4731-a039-f885317c4538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load data\n",
        "print('----preparing data------')\n",
        "\n",
        "with open (\"/content/drive/My Drive/deep_learning/dl_research/interpretable_RNN/amazon/train_vects\", 'rb') as fp:\n",
        "  train_vects = pickle.load(fp)\n",
        "\n",
        "with open (\"./drive/My Drive/deep_learning/dl_research/interpretable_RNN/amazon/y_train\", 'rb') as fp:\n",
        "  y_train = pickle.load(fp)\n",
        "\n",
        "with open ('./drive/My Drive/deep_learning/dl_research/interpretable_RNN/amazon/train_clean', 'rb') as fp:\n",
        "    train_clean = pickle.load(fp)\n",
        "\n",
        "with open ('./drive/My Drive/deep_learning/dl_research/interpretable_RNN/amazon/train_not_clean', 'rb') as fp:\n",
        "    train_not_clean = pickle.load(fp)\n",
        "\n",
        "y_train_full = np.array(y_train)\n",
        "y_train = np.array(y_train)\n",
        "y_train = np.expand_dims(y_train, axis=1)\n"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----preparing data------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZKFeSU33Wmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(len(train_vects),len(train_vects[0]))\n",
        "# print(len(y_train),y_train[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHaf4Q7lmras",
        "colab_type": "code",
        "outputId": "e51ef65b-b052-4806-eefe-7674b741b22b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "data_size = 3000\n",
        "# valid_size = 1000\n",
        "def get_batches(x, y,batch_size):\n",
        "    '''Create the batches for the training and validation data'''\n",
        "    n_batches = len(x)//batch_size\n",
        "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
        "    for ii in range(0, len(x), batch_size):\n",
        "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
        "print(len(train_vects), len(train_vects[1]))\n",
        "print(len(y_train))\n",
        "print(y_train[0])\n",
        "\n",
        "num_folds = 5\n",
        "kFold = 0\n",
        "\n",
        "X_train_folds = np.array_split(train_vects[:data_size], num_folds)\n",
        "y_train_folds = np.array_split(y_train[:data_size], num_folds)\n",
        "\n",
        "x_train, x_valid = [], list(X_train_folds[kFold])\n",
        "y_train, y_valid = [], list(y_train_folds[kFold])\n",
        "for i in range(0,num_folds):\n",
        "    if i != kFold:\n",
        "        x_train.extend(X_train_folds[i])\n",
        "        y_train.extend(y_train_folds[i])\n",
        "\n",
        "\n",
        "print('x_train = ',len(x_train),\u001dlen(x_train[1]),len(x_train[1][0]))\n",
        "def check_dist(a):\n",
        "  return np.array([np.percentile(a,i) for i in [0,25,50,75,100]])\n",
        "def np_distances(cents,points):\n",
        "  p1 = np.tile(points, (cents.shape[0], 1)).reshape(cents.shape[0],points.shape[0],points.shape[1])\n",
        "  c1 = np.expand_dims(cents,axis=1)\n",
        "  d1 = p1 - c1\n",
        "  d2 = d1*d1\n",
        "  d3 = np.sum(d2,axis=2)\n",
        "  return d3\n",
        "def match_sents(k_cents,all_sents):\n",
        "  # res = []\n",
        "  dProsSents = np_distances(k_cents,np.array(all_sents))\n",
        "  dProsSentsArgSort = np.argsort(dProsSents,axis=1)\n",
        "  return all_sents[dProsSentsArgSort[:,0]]\n",
        "\n",
        "print(check_dist([-5,2,10,0,1,5,4,6,7,-4,-3,8]))\n",
        "print(check_dist([-1,0.5,1,0.7,-0.8,-0.5,-0.2,0.8, 0.3]))\n",
        "# print('y_train: ',y_train.shape)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30001 5\n",
            "30001\n",
            "[1]\n",
            "x_train =  2400 2 768\n",
            "[-5.   -0.75  3.    6.25 10.  ]\n",
            "[-1.  -0.5  0.3  0.7  1. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGBgvEtqmt3d",
        "colab_type": "code",
        "outputId": "cfd3dce7-c1a0-4c00-fae7-279edc04a69c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#----clustering-----\n",
        "all_sents = [sent for para in x_train for sent in para]\n",
        "k_protos = 20\n",
        "print(len(all_sents),len(all_sents[0]))\n",
        "kmedoids = KMedoids(n_clusters=k_protos,random_state=0).fit(all_sents[0:10000])\n",
        "# kmedoids = KMedoids(n_clusters=k_protos,metric='cosine',random_state=0).fit(all_sents[0:10000])\n",
        "k_cents = kmedoids.cluster_centers_\n",
        "print(k_cents.shape)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14953 768\n",
            "(20, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH25ma8C5VCY",
        "colab_type": "code",
        "outputId": "c3ec4c7b-d6ba-46aa-df3a-7d97d614cb79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "for c in k_cents:\n",
        "  print(check_dist(c))"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.21362066 -0.36394682 -0.00421334  0.35230722  2.2496078 ]\n",
            "[-2.24929309 -0.36212804  0.01551494  0.321096    1.78031921]\n",
            "[-1.85124207 -0.34618915  0.00813584  0.33220163  2.2175765 ]\n",
            "[-3.07907462e+00 -3.41143712e-01 -2.45070679e-03  3.15083027e-01\n",
            "  1.84032619e+00]\n",
            "[-1.86753654 -0.386037   -0.00364652  0.36949214  1.88630879]\n",
            "[-1.89380932e+00 -4.50098388e-01 -2.30933505e-03  3.88980456e-01\n",
            "  2.44710279e+00]\n",
            "[-3.03781986 -0.3639546  -0.02061732  0.33252376  2.41140342]\n",
            "[-2.53508568 -0.3230486  -0.0069954   0.35568865  2.1769278 ]\n",
            "[-2.37569451 -0.37786707 -0.00452666  0.36098225  2.08828235]\n",
            "[-2.32970691 -0.36985804 -0.01914496  0.31179453  2.02894163]\n",
            "[-2.5167172  -0.34546048  0.01047673  0.35073632  1.87700129]\n",
            "[-1.77577424 -0.33705235 -0.03569043  0.30009926  1.81651294]\n",
            "[-2.23104453e+00 -3.33338164e-01  2.06786237e-03  3.21544953e-01\n",
            "  1.72285604e+00]\n",
            "[-2.06194615 -0.38023037  0.01769366  0.35381508  1.52218342]\n",
            "[-2.70944953e+00 -3.49739827e-01 -3.51661947e-04  3.37845221e-01\n",
            "  2.02874708e+00]\n",
            "[-2.80676746e+00 -4.24200259e-01 -1.77410326e-03  4.12372373e-01\n",
            "  2.23701358e+00]\n",
            "[-2.32136464 -0.36439244 -0.0033754   0.3624427   1.67061651]\n",
            "[-3.10115051 -0.32841893 -0.01694602  0.31573427  1.53870368]\n",
            "[-2.76244044 -0.36445671 -0.01257486  0.33169797  1.73948288]\n",
            "[-2.17382431 -0.38853146 -0.02511555  0.34664726  2.15224075]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZtijp5HoDZp",
        "colab_type": "text"
      },
      "source": [
        "#Build and train model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su9kF5C7oLbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_cell(lstm_size, keep_prob):\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
        "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
        "    return drop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5cxMbVToO4L",
        "colab_type": "code",
        "outputId": "e8390f65-5d83-4995-a363-a5ddc44cde55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# The default parameters of the model\n",
        "# n_words = len(word_index)\n",
        "n_words = 1\n",
        "embed_size = 300\n",
        "batch_size = 1\n",
        "lstm_size = 128\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "multiple_fc = False\n",
        "fc_units = 256\n",
        "vect_size = 768\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Declare placeholders we'll feed into the graph\n",
        "with tf.name_scope('inputs'):\n",
        "    inputs = tf.placeholder(tf.float32, [batch_size, None, vect_size], name='inputs')\n",
        "    print('[db] inputs = ', inputs)\n",
        "\n",
        "with tf.name_scope('labels'):\n",
        "    labels = tf.placeholder(tf.int32, [batch_size, None], name='labels')\n",
        "\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "\n",
        "# Build the RNN layers\n",
        "with tf.name_scope(\"RNN_layers\"):\n",
        "    cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
        "\n",
        "# Set the initial state\n",
        "with tf.name_scope(\"RNN_init_state\"):\n",
        "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "\n",
        "# prototypes\n",
        "with tf.name_scope(\"prototypes\"):\n",
        "    protos_values = tf.placeholder(tf.float32, [k_protos, vect_size], name='protos_values')\n",
        "\n",
        "    prototypes = tf.get_variable('prototypes', shape=[k_protos, vect_size],\n",
        "                                 initializer=tf.constant_initializer(k_cents[:k_protos]), trainable=True)\n",
        "    print('[db] prototypes = ', prototypes)\n",
        "\n",
        "    proto_assign_op = prototypes.assign(protos_values)\n",
        "    print('[db] proto_assign_op = ', proto_assign_op)\n",
        "\n",
        "\n",
        "\n",
        "def compute_distance(outputs, p, n_protos):\n",
        "    tmp1 = tf.expand_dims(outputs, 2)\n",
        "    tmp1 = tf.broadcast_to(tmp1, [tmp1.shape[0].value, tf.shape(tmp1)[1], n_protos, vect_size])\n",
        "\n",
        "    tmp2 = tf.broadcast_to(p, [tmp1.shape[0].value, tf.shape(tmp1)[1], n_protos, vect_size])\n",
        "\n",
        "    tmp3 = tmp1 - tmp2\n",
        "    tmp4 = tmp3 * tmp3\n",
        "    distances = tf.reduce_sum(tmp4, axis=3)\n",
        "    return distances, tf.reduce_min(distances, axis=1)\n",
        "\n",
        "def e_func(x,a = 0.1, e=2.7182818284590452353602874713527):\n",
        "    return tf.math.pow(e,-(a*x))\n",
        "\n",
        "full_distances, full_min_dist = compute_distance(inputs, prototypes, k_protos)\n",
        "\n",
        "\n",
        "min_dist_ind = tf.nn.softmax(-full_distances  * beta)\n",
        "\n",
        "dist_hot_vect = min_dist_ind * e_func(tf.math.sqrt(full_distances + 1e-8))\n",
        "\n",
        "with tf.name_scope(\"RNN_forward\"):\n",
        "\n",
        "    outputs, final_state = tf.nn.dynamic_rnn(cell, dist_hot_vect, initial_state=initial_state)\n",
        "\n",
        "\n",
        "# Create the fully connected layers\n",
        "with tf.name_scope(\"fully_connected\"):\n",
        "    # Initialize the weights and biases\n",
        "    weights = tf.truncated_normal_initializer(stddev=0.1)\n",
        "    biases = tf.zeros_initializer()\n",
        "\n",
        "    dense = tf.contrib.layers.fully_connected(outputs[:, -1], num_outputs=fc_units, activation_fn=tf.sigmoid,\n",
        "                                              weights_initializer=weights, biases_initializer=biases)\n",
        "    dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
        "\n",
        "    # Depending on the iteration, use a second fully connected layer\n",
        "    if multiple_fc == True:\n",
        "        dense = tf.contrib.layers.fully_connected(dense, num_outputs=fc_units, activation_fn=tf.sigmoid,\n",
        "                                                  weights_initializer=weights, biases_initializer=biases)\n",
        "        dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
        "\n",
        "# Make the predictions\n",
        "with tf.name_scope('predictions'):\n",
        "    predictions = tf.contrib.layers.fully_connected(dense,\n",
        "                                                    num_outputs=2,\n",
        "                                                    activation_fn=tf.sigmoid,\n",
        "                                                    weights_initializer=weights,\n",
        "                                                    biases_initializer=biases)\n",
        "    predictions = predictions + 1e-8\n",
        "    tf.summary.histogram('predictions', predictions)\n",
        "\n",
        "# Calculate the cost\n",
        "with tf.name_scope('cost'):\n",
        "    \n",
        "    def tight_pos_sigmoid_offset(x, offset, e=2.7182818284590452353602874713527):\n",
        "\t      return 1 / (1 + tf.math.pow(e, (1 * (offset*x-0.5))))\n",
        "\n",
        "\n",
        "    squared_cost = tf.losses.mean_squared_error(labels, predictions) + 1e-8\n",
        "\n",
        "    def pw_distance(A):\n",
        "        # A = tf.constant([[1, 1], [4, 4], [5, 5],[6,6]])\n",
        "        r = tf.reduce_sum(A * A, 1)\n",
        "        print('[db] r = ', r)\n",
        "        # turn r into column vector\n",
        "        r = tf.reshape(r, [-1, 1])\n",
        "        D = r - 2 * tf.matmul(A, tf.transpose(A)) + tf.transpose(r)\n",
        "        return D\n",
        "\n",
        "\n",
        "    costR_d = tf.reduce_sum(tf.reduce_min(full_distances,axis=2))\n",
        "    \n",
        "    d = pw_distance(prototypes ) \n",
        "    \n",
        "    diag_ones = tf.convert_to_tensor(np.eye(pos_pro_k + neg_pro_k, dtype=float))\n",
        "    diag_ones = tf.dtypes.cast(diag_ones, tf.float32)\n",
        "    d1 = d + diag_ones * tf.reduce_max(d) \n",
        "    d2 = tf.reduce_min(d1, axis=1) \n",
        "    min_d2_dist = tf.reduce_min(d2) \n",
        "    costC = tight_pos_sigmoid_offset(min_d2_dist, 1) + 1e-8\n",
        "\n",
        "    cost = squared_cost + 0.0001 * costR_d + 0.1 * costC\n",
        "\n",
        "\n",
        "    \n",
        "    # test_var = [full_distances,e_func(full_distances),dist_hot_vect]\n",
        "    test_var = [costC]\n",
        "\n",
        "\n",
        "# Train the model\n",
        "with tf.name_scope('train'):\n",
        "    lr1, lr2 = 0.0001, 0.0005\n",
        "    optimizer = tf.train.AdamOptimizer(lr1).minimize(cost)\n",
        "\n",
        "    # optimizer1 = tf.train.AdamOptimizer(lr1).minimize(cost1)\n",
        "    # optimizer2 = tf.train.AdamOptimizer(lr2).minimize(cost2)\n",
        "\n",
        "# Determine the accuracy\n",
        "with tf.name_scope(\"accuracy\"):\n",
        "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n",
        "        all_labels_true = tf.reduce_min(tf.cast(correct_pred, tf.float32),1)\n",
        "        # accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "        accuracy = tf.reduce_mean(all_labels_true)\n",
        "\n",
        "\n"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[db] inputs =  Tensor(\"inputs/inputs:0\", shape=(1, ?, 768), dtype=float32)\n",
            "[db] prototypes =  <tf.Variable 'prototypes_1:0' shape=(20, 768) dtype=float32_ref>\n",
            "[db] proto_assign_op =  Tensor(\"prototypes/Assign:0\", shape=(20, 768), dtype=float32_ref)\n",
            "[db] r =  Tensor(\"cost/Sum_1:0\", shape=(20,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e6V8EHAqynf",
        "colab_type": "code",
        "outputId": "2e5aff75-d7b1-49ff-de81-5a0ac72792b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "log_string = 'ru={},fcl={},fcu={}'.format(lstm_size,\n",
        "                                                      multiple_fc,\n",
        "                                                      fc_units)   \n",
        "recorded_train_acc, recorded_valid_acc = [], []     \n",
        "with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        valid_loss_summary = []\n",
        "        \n",
        "        # Keep track of which batch iteration is being trained\n",
        "        iteration = 0\n",
        "\n",
        "        avg_train_acc = 0\n",
        "        \n",
        "        with open('_prototypes', 'wb') as fp:\n",
        "          pickle.dump(k_cents, fp)\n",
        "        with open ('_prototypes', 'rb') as fp:\n",
        "          _prototypes = pickle.load(fp)\n",
        "              \n",
        "        for e in range(epochs):\n",
        "            state = sess.run(initial_state)\n",
        "            \n",
        "            # Record progress with each epoch\n",
        "            train_loss, train_acc, val_acc, val_loss = [], [], [], []\n",
        "            # optimizer = optimizer1\n",
        "            print('[db] Using opt1')\n",
        "            # if avg_train_acc > 0.65:\n",
        "            #   print('[db] Using opt2')\n",
        "            #   optimizer = model.optimizer2\n",
        "\n",
        "            with tqdm(total=len(x_train)) as pbar:\n",
        "                for i, (x, y) in enumerate(get_batches(x_train, y_train, batch_size), 1):\n",
        "\n",
        "                    feed = {inputs: x,\n",
        "                            protos_values: _prototypes,      \n",
        "                            labels: y,\n",
        "                            keep_prob: dropout,\n",
        "                            initial_state: state,\n",
        "                            }\n",
        "                    loss, \\\n",
        "                    acc, \\\n",
        "                    state,\\\n",
        "                    _, \\\n",
        "                    _prototypes, \\\n",
        "                    _predictions, \\\n",
        "                    _test_var = sess.run([\n",
        "                                                             cost, \n",
        "                                                             accuracy, \n",
        "                                                             final_state, \n",
        "                                                             optimizer,\n",
        "                                                             prototypes,\n",
        "                                                             predictions,\n",
        "                                                             test_var\n",
        "                                                            ], \n",
        "                                                           feed_dict=feed)\n",
        "    \n",
        "                    if i % 1000 == 0:\n",
        "                      # print(\"test_var: \",i)\n",
        "                      for v in _test_var:\n",
        "                        print('v = ',v,v.shape)\n",
        "                      with open('./test_var', 'wb') as fp:\n",
        "                        pickle.dump(_test_var, fp)\n",
        "                      # print(\"_neg_prototypes : \",_neg_prototypes)\n",
        "                    # Record the loss and accuracy of each training batch\n",
        "                    train_loss.append(loss)\n",
        "                    train_acc.append(acc)                  \n",
        "                    # Record the progress of training\n",
        "                    \n",
        "                    iteration += 1\n",
        "                    pbar.update(batch_size)\n",
        "\n",
        "            # --- projection\n",
        "            if e % 10 == 0:\n",
        "              new_protos = match_sents(_prototypes,np.array(all_sents[:10000]))\n",
        "\n",
        "              _prototypes, _prototypes_values, _  = sess.run([prototypes, protos_values, proto_assign_op], feed_dict={protos_values: new_protos})\n",
        "\n",
        "              print(_prototypes)\n",
        "              print('Saving projected protos...')\n",
        "              with open('_prototypes', 'wb') as fp:\n",
        "                    pickle.dump(_prototypes, fp)\n",
        "\n",
        "              with open(\"./drive/My Drive/deep_learning/dl_research/interpretable_RNN/vars/_prototypes\", 'wb') as fp:\n",
        "                    pickle.dump(_prototypes, fp)\n",
        "              checkpoint = \"./sentiment_{}.ckpt\".format(log_string)\n",
        "              saver.save(sess, checkpoint)\n",
        "            # -----\n",
        "\n",
        "            # Average the training loss and accuracy of each epoch\n",
        "            avg_train_loss = np.mean(train_loss)\n",
        "            avg_train_acc = np.mean(train_acc)\n",
        "            \n",
        "            recorded_train_acc.append(avg_train_acc)\n",
        "\n",
        "            val_state = sess.run(initial_state)\n",
        "            with tqdm(total=len(x_valid)) as pbar:\n",
        "                for x, y in get_batches(x_valid, y_valid, batch_size):\n",
        "\n",
        "                    feed = {inputs: x,\n",
        "                            protos_values: _prototypes,\n",
        "                            # model.labels: np.squeeze(y,axis=2),\n",
        "                            labels: y,\n",
        "                            keep_prob: 1,\n",
        "                            initial_state: val_state,\n",
        "                            # model.pretrained_embedding: embedding_matrix\n",
        "                            }\n",
        "                    batch_loss, \\\n",
        "                    batch_acc, \\\n",
        "                    val_state, \\\n",
        "                    _prototypes, \\\n",
        "                    _predictions  = sess.run([ \n",
        "                                                                          cost, \n",
        "                                                                          accuracy, \n",
        "                                                                          final_state,\n",
        "                                                                          prototypes,\n",
        "                                                                          predictions\n",
        "                                                                          ], \n",
        "                                                                         feed_dict=feed)\n",
        "                    \n",
        "                    # Record the validation loss and accuracy of each epoch\n",
        "                    val_loss.append(batch_loss)\n",
        "                    val_acc.append(batch_acc)\n",
        "\n",
        "                    pbar.update(batch_size)\n",
        "            \n",
        "            # Average the validation loss and accuracy of each epoch\n",
        "            avg_valid_loss = np.mean(val_loss)    \n",
        "            avg_valid_acc = np.mean(val_acc)\n",
        "\n",
        "            recorded_valid_acc.append(avg_valid_acc)\n",
        "\n",
        "\n",
        "            valid_loss_summary.append(avg_valid_loss)\n",
        "            \n",
        "            # Record the validation data's progress\n",
        "\n",
        "            print(\"Epoch: {}/{}\".format(e, epochs))\n",
        "            print(\"Train Loss: {:.3f}\".format(avg_train_loss))\n",
        "            print(\"Train Acc: {:.3f}\".format(avg_train_acc))\n",
        "            print(\"Valid Loss: {:.3f}\".format(avg_valid_loss))\n",
        "            print(\"Valid Acc: {:.3f}\".format(avg_valid_acc))\n",
        "\n",
        "            with open('_recorded_train_acc', 'wb') as fp:\n",
        "                    pickle.dump(recorded_train_acc, fp)\n",
        "            with open('_recorded_valid_acc', 'wb') as fp:\n",
        "                    pickle.dump(recorded_valid_acc, fp)\n",
        "\n",
        "            # Stop training if the validation loss does not decrease after 3 epochs\n",
        "            if avg_valid_loss > min(valid_loss_summary):\n",
        "                print(\"No Improvement.\")\n",
        "                stop_early += 1\n",
        "                if stop_early == 100:\n",
        "                    break   \n",
        "            \n",
        "            # Reset stop_early if the validation loss finds a new low\n",
        "            # Save a checkpoint of the model\n",
        "            else:\n",
        "                print(\"New Record!\")\n",
        "                stop_early = 0\n",
        "                checkpoint = \"./sentiment_{}.ckpt\".format(log_string)\n",
        "                saver.save(sess, checkpoint)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/2400 [00:00<06:25,  6.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1009/2400 [00:11<00:15, 87.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2012/2400 [00:23<00:04, 96.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:27<00:00, 86.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 0.11107484 -0.097921    1.3381573  ... -0.34753498 -0.4679819\n",
            "   0.11056099]\n",
            " [ 0.11331315 -0.10886213  0.61135334 ...  0.09124227 -0.35262236\n",
            "   0.60027975]\n",
            " [ 0.26907867  1.0641567   1.1225679  ...  0.3846132  -0.36755037\n",
            "   0.24458894]\n",
            " ...\n",
            " [-0.3516928   0.624261    1.1217483  ...  0.37640408  0.17623821\n",
            "  -0.26328892]\n",
            " [ 0.27060717 -0.01896457  1.4450957  ...  0.39388338 -0.94533986\n",
            "   0.15132998]\n",
            " [-0.00850752  0.5792342   0.3424907  ...  0.34219983 -0.12921074\n",
            "   0.06783759]]\n",
            "Saving projected protos...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [00:02<00:00, 261.64it/s]\n",
            "  0%|          | 0/2400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/100\n",
            "Train Loss: 0.381\n",
            "Train Acc: 0.317\n",
            "Valid Loss: 0.407\n",
            "Valid Acc: 0.490\n",
            "New Record!\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1009/2400 [00:11<00:15, 88.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2014/2400 [00:23<00:04, 93.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:27<00:00, 87.44it/s]\n",
            "100%|██████████| 600/600 [00:01<00:00, 314.25it/s]\n",
            "  0%|          | 0/2400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/100\n",
            "Train Loss: 0.375\n",
            "Train Acc: 0.320\n",
            "Valid Loss: 0.386\n",
            "Valid Acc: 0.490\n",
            "New Record!\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1013/2400 [00:11<00:16, 85.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2015/2400 [00:23<00:04, 95.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:27<00:00, 87.26it/s]\n",
            "100%|██████████| 600/600 [00:01<00:00, 311.17it/s]\n",
            "  0%|          | 0/2400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2/100\n",
            "Train Loss: 0.356\n",
            "Train Acc: 0.348\n",
            "Valid Loss: 0.380\n",
            "Valid Acc: 0.490\n",
            "New Record!\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1014/2400 [00:11<00:15, 88.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2013/2400 [00:23<00:03, 98.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:27<00:00, 87.33it/s]\n",
            "100%|██████████| 600/600 [00:01<00:00, 308.03it/s]\n",
            "  0%|          | 9/2400 [00:00<00:29, 81.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3/100\n",
            "Train Loss: 0.341\n",
            "Train Acc: 0.411\n",
            "Valid Loss: 0.381\n",
            "Valid Acc: 0.497\n",
            "No Improvement.\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1016/2400 [00:11<00:15, 87.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2016/2400 [00:23<00:04, 94.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:27<00:00, 87.62it/s]\n",
            "100%|██████████| 600/600 [00:01<00:00, 315.81it/s]\n",
            "  0%|          | 0/2400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4/100\n",
            "Train Loss: 0.300\n",
            "Train Acc: 0.572\n",
            "Valid Loss: 0.356\n",
            "Valid Acc: 0.570\n",
            "New Record!\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1015/2400 [00:11<00:16, 86.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2010/2400 [00:23<00:04, 94.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:27<00:00, 86.85it/s]\n",
            "100%|██████████| 600/600 [00:01<00:00, 310.53it/s]\n",
            "  0%|          | 0/2400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/100\n",
            "Train Loss: 0.240\n",
            "Train Acc: 0.740\n",
            "Valid Loss: 0.245\n",
            "Valid Acc: 0.768\n",
            "New Record!\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1008/2400 [00:11<00:16, 86.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2017/2400 [00:23<00:04, 90.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1.0000001e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:27<00:00, 86.40it/s]\n",
            "100%|██████████| 600/600 [00:01<00:00, 308.33it/s]\n",
            "  0%|          | 0/2400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 6/100\n",
            "Train Loss: 0.225\n",
            "Train Acc: 0.769\n",
            "Valid Loss: 0.211\n",
            "Valid Acc: 0.818\n",
            "New Record!\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1010/2400 [00:11<00:16, 84.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1.0000003e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2013/2400 [00:22<00:04, 92.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1.0000007e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:27<00:00, 88.00it/s]\n",
            "100%|██████████| 600/600 [00:01<00:00, 310.06it/s]\n",
            "  0%|          | 9/2400 [00:00<00:29, 81.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 7/100\n",
            "Train Loss: 0.214\n",
            "Train Acc: 0.790\n",
            "Valid Loss: 0.237\n",
            "Valid Acc: 0.770\n",
            "No Improvement.\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1015/2400 [00:11<00:16, 84.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1.000002e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2016/2400 [00:23<00:04, 91.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1.0000039e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:27<00:00, 87.20it/s]\n",
            "100%|██████████| 600/600 [00:01<00:00, 301.51it/s]\n",
            "  0%|          | 0/2400 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 8/100\n",
            "Train Loss: 0.208\n",
            "Train Acc: 0.793\n",
            "Valid Loss: 0.201\n",
            "Valid Acc: 0.843\n",
            "New Record!\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1016/2400 [00:11<00:15, 86.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1.0000078e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2011/2400 [00:23<00:04, 96.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1.0000117e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:27<00:00, 87.31it/s]\n",
            "100%|██████████| 600/600 [00:01<00:00, 317.30it/s]\n",
            "  0%|          | 9/2400 [00:00<00:28, 83.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 9/100\n",
            "Train Loss: 0.203\n",
            "Train Acc: 0.805\n",
            "Valid Loss: 0.201\n",
            "Valid Acc: 0.828\n",
            "No Improvement.\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1007/2400 [00:11<00:15, 89.82it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1.0000191e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2011/2400 [00:22<00:04, 93.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1.000024e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:27<00:00, 88.01it/s]\n",
            "  0%|          | 0/600 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 0.11107484 -0.097921    1.3381573  ... -0.34753498 -0.4679819\n",
            "   0.11056099]\n",
            " [ 0.11331315 -0.10886213  0.61135334 ...  0.09124227 -0.35262236\n",
            "   0.60027975]\n",
            " [ 0.26907867  1.0641567   1.1225679  ...  0.3846132  -0.36755037\n",
            "   0.24458894]\n",
            " ...\n",
            " [-0.3516928   0.624261    1.1217483  ...  0.37640408  0.17623821\n",
            "  -0.26328892]\n",
            " [ 0.27060717 -0.01896457  1.4450957  ...  0.39388338 -0.94533986\n",
            "   0.15132998]\n",
            " [-0.00850752  0.5792342   0.3424907  ...  0.34219983 -0.12921074\n",
            "   0.06783759]]\n",
            "Saving projected protos...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [00:01<00:00, 311.53it/s]\n",
            "  0%|          | 8/2400 [00:00<00:30, 77.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10/100\n",
            "Train Loss: 0.201\n",
            "Train Acc: 0.809\n",
            "Valid Loss: 0.254\n",
            "Valid Acc: 0.823\n",
            "No Improvement.\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1013/2400 [00:12<00:17, 78.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2016/2400 [00:23<00:04, 94.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:28<00:00, 85.45it/s]\n",
            "100%|██████████| 600/600 [00:01<00:00, 309.01it/s]\n",
            "  0%|          | 9/2400 [00:00<00:29, 81.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 11/100\n",
            "Train Loss: 0.246\n",
            "Train Acc: 0.776\n",
            "Valid Loss: 0.243\n",
            "Valid Acc: 0.827\n",
            "No Improvement.\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1010/2400 [00:11<00:16, 86.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 2015/2400 [00:23<00:04, 93.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:27<00:00, 87.36it/s]\n",
            "100%|██████████| 600/600 [00:01<00:00, 314.43it/s]\n",
            "  0%|          | 7/2400 [00:00<00:36, 66.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 12/100\n",
            "Train Loss: 0.233\n",
            "Train Acc: 0.796\n",
            "Valid Loss: 0.233\n",
            "Valid Acc: 0.822\n",
            "No Improvement.\n",
            "[db] Using opt1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 1008/2400 [00:11<00:17, 81.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "v =  1e-08 ()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 1141/2400 [00:13<00:14, 83.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-d10adf76afd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                                                              \u001b[0mtest_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                                             ], \n\u001b[0;32m---> 49\u001b[0;31m                                                            feed_dict=feed)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duF8c0bLBs1H",
        "colab_type": "text"
      },
      "source": [
        "#Show the prototypes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p33O1Odn77L1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('./_prototypes', 'rb') as fp:\n",
        "\t_prototypes = pickle.load(fp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6aXOs9QCkAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "2c15aa6a-5e9c-4d0e-95ae-6809d1df7c10"
      },
      "source": [
        "print('protos: ',_prototypes.shape)\n",
        "for p in _prototypes:\n",
        "  print(check_dist(p))\n",
        "print('y_train_full: ',y_train_full.shape)"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "protos:  (20, 768)\n",
            "[-2.21362066 -0.36394682 -0.00421334  0.35230722  2.2496078 ]\n",
            "[-2.24929309 -0.36212804  0.01551494  0.321096    1.78031921]\n",
            "[-1.85124207 -0.34618915  0.00813584  0.33220163  2.2175765 ]\n",
            "[-3.07907462e+00 -3.41143712e-01 -2.45070679e-03  3.15083027e-01\n",
            "  1.84032619e+00]\n",
            "[-1.86753654 -0.386037   -0.00364652  0.36949214  1.88630879]\n",
            "[-1.89380932e+00 -4.50098388e-01 -2.30933505e-03  3.88980456e-01\n",
            "  2.44710279e+00]\n",
            "[-3.03781986 -0.3639546  -0.02061732  0.33252376  2.41140342]\n",
            "[-2.53508568 -0.3230486  -0.0069954   0.35568865  2.1769278 ]\n",
            "[-2.37569451 -0.37786707 -0.00452666  0.36098225  2.08828235]\n",
            "[-2.32970691 -0.36985804 -0.01914496  0.31179453  2.02894163]\n",
            "[-2.5167172  -0.34546048  0.01047673  0.35073632  1.87700129]\n",
            "[-1.77577424 -0.33705235 -0.03569043  0.30009926  1.81651294]\n",
            "[-2.23104453e+00 -3.33338164e-01  2.06786237e-03  3.21544953e-01\n",
            "  1.72285604e+00]\n",
            "[-2.06194615 -0.38023037  0.01769366  0.35381508  1.52218342]\n",
            "[-2.70944953e+00 -3.49739827e-01 -3.51661947e-04  3.37845221e-01\n",
            "  2.02874708e+00]\n",
            "[-2.79939103 -0.42698356  0.01330372  0.41931923  2.20841074]\n",
            "[-2.32136464 -0.36439244 -0.0033754   0.3624427   1.67061651]\n",
            "[-3.10115051 -0.32841893 -0.01694602  0.31573427  1.53870368]\n",
            "[-2.76244044 -0.36445671 -0.01257486  0.33169797  1.73948288]\n",
            "[-2.17382431 -0.38853146 -0.02511555  0.34664726  2.15224075]\n",
            "y_train_full:  (30001,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c35lPfPHUvWl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97ae2fc5-9494-4705-d4ff-6fd91cc8cdaa"
      },
      "source": [
        "print(len(x_train),len(y_train))"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2400 2400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5irf77LbCYwJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "479eaf46-8b58-4036-d5f2-9c90d8334bca"
      },
      "source": [
        "p_count = 0\n",
        "d_pos = {}\n",
        "for p_count,p in enumerate(_prototypes):\n",
        "  print('p_count = ',p_count)\n",
        "  s_count = 0\n",
        "  d_pos[p_count] = {}\n",
        "  for i,para in enumerate(train_vects[:data_size]):\n",
        "    for j,s in enumerate(para):\n",
        "      if len(train_clean[i].split(\".\")[j]) < 5:\n",
        "        continue\n",
        "      d_pos[p_count][(i,j)] = np.linalg.norm(train_vects[i][j]-p)\n",
        "      # print('i,j = ',i,j)\n",
        "      # d_pos[p_count][(i,j)] = spatial.distance.cosine(train_vects[i][j], p)\n",
        "      s_count+=1\n",
        "  print('count = ',s_count)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p_count =  0\n",
            "count =  18650\n",
            "p_count =  1\n",
            "count =  18650\n",
            "p_count =  2\n",
            "count =  18650\n",
            "p_count =  3\n",
            "count =  18650\n",
            "p_count =  4\n",
            "count =  18650\n",
            "p_count =  5\n",
            "count =  18650\n",
            "p_count =  6\n",
            "count =  18650\n",
            "p_count =  7\n",
            "count =  18650\n",
            "p_count =  8\n",
            "count =  18650\n",
            "p_count =  9\n",
            "count =  18650\n",
            "p_count =  10\n",
            "count =  18650\n",
            "p_count =  11\n",
            "count =  18650\n",
            "p_count =  12\n",
            "count =  18650\n",
            "p_count =  13\n",
            "count =  18650\n",
            "p_count =  14\n",
            "count =  18650\n",
            "p_count =  15\n",
            "count =  18650\n",
            "p_count =  16\n",
            "count =  18650\n",
            "p_count =  17\n",
            "count =  18650\n",
            "p_count =  18\n",
            "count =  18650\n",
            "p_count =  19\n",
            "count =  18650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QyJ2djTDRV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "10680c1e-f302-4fca-96d5-2d0cad086d3e"
      },
      "source": [
        "import operator\n",
        "protos_sents = {}\n",
        "k_closest_sents = 5\n",
        "recorded_protos_score = {}\n",
        "for l in range(pos_pro_k + neg_pro_k):\n",
        "  print(\"prototype index = \",l)\n",
        "  recorded_protos_score[l] = {}\n",
        "  sorted_d = sorted(d_pos[l].items(), key=operator.itemgetter(1))\n",
        "  proto_scores = []\n",
        "  closest_sents = []\n",
        "  for k in range(k_closest_sents):\n",
        "    (i,j) = sorted_d[k][0]\n",
        "    print(sorted_d[k], y_train_full[i], train_not_clean[i].split(\".\")[j])\n",
        "    # print()\n",
        "    proto_scores.append(int(y_train_full[i]))\n",
        "    # closest_sents.append(train_not_clean[i].split(\".\")[j] + \" \" + str(y_train_full[i]))\n",
        "  (m,n) = sorted_d[0][0] \n",
        "  protos_sents[l] = (train_not_clean[m].split(\".\")[n],y_train_full[i],sum(proto_scores)/len(proto_scores),closest_sents)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prototype index =  0\n",
            "((2135, 2), 0.0) 0  Even when it did work it was a hit or miss thing\n",
            "((272, 7), 10.323638) 0 95 above and beyond the already rather pricy product\n",
            "((701, 3), 10.72252) 0  I did, although it was unrewarding to do so\n",
            "((2746, 0), 10.749016) 1 This is made well and strudy, but will wear out\n",
            "((1367, 5), 10.792902) 0  Once it was on it really didn't do what it is supposed to do - it's more awkward than anything\n",
            "prototype index =  1\n",
            "((1581, 4), 0.0) 0 While certain parts of this story are interesting, in it's entirety, i would have to give it two thumbs down (\n",
            "((1339, 2), 9.720231) 0 You'll find a couple of interesting ideas, a lot of trite and derivative ones and occasionally some baffling lack of coherent character development\n",
            "((2617, 5), 9.727579) 0  But considering the ending seemed to be rather ill-fitting to the rest of the tale, I'll have to pass on giving it a high rating\n",
            "((102, 5), 10.011818) 1  While she sometimes seems too guillible, I have to remember that it is part of the story\n",
            "((796, 5), 10.180044) 0  While this book has a rather good story it`s concealed by endless symbols and references, I suggest the cliff notes version\n",
            "prototype index =  2\n",
            "((1158, 1), 0.0) 0  I'm sure this product is great for G4 Powebooks and G3 iBooks, but nowhere in this product description does it say that this adapter is NOT for use with iBook G4s\n",
            "((1172, 2), 10.286843) 0  After looking at mine i noticed i too had been slipped the Inspire adapter, not the Macally that i ordered (which is far superior to the cheap knock off)\n",
            "((1211, 1), 10.364422) 1  Although this is a wonderful working power adapter, I don't believe it is the Macally Adapter that was advertised\n",
            "((2450, 2), 10.453482) 0  the reason i am giving it 2 stars all it had to offer was a track not included in the box set which is the come as you are demo\n",
            "((2048, 1), 10.461509) 1  The mugs are very nice, but I guess I didn't think of the 16 oz\n",
            "prototype index =  3\n",
            "((970, 2), 0.0) 0  The only redeeming features are a few memorable songs, such as \"I'll Be Back\" and \"Stop, Look and Listen\n",
            "((2008, 4), 10.362297) 1 \"Billy\" is instantly likeable, while songs like \"Shine On\" and \"Made in Heaven\" are the slower favourites\n",
            "((2394, 1), 10.538877) 0  I'm sure I am one of a select few that do not appreciate a story that develops great characters and great sub-plots and then ends abruptly\n",
            "((2577, 2), 10.650875) 0  Such a shame that most of these compilations only ever have a few great inspiring tracks while the rest are just pure background stuff\n",
            "((1288, 7), 10.722392) 0  It's usually competent, even musical, but when an occasional vocal bit is all that differentiates one track from another, one tends to seek greener electropastures\n",
            "prototype index =  4\n",
            "((1725, 2), 0.0) 1  Not the kind of book I usually read, although I'm glad I took the time to give this one a whirl\n",
            "((896, 2), 7.623134) 1  This is not a book I would normally have picked up and read, but I am glad that I read it\n",
            "((837, 1), 8.826056) 0  I usually have nothing to say but good things about book I read\n",
            "((2631, 2), 9.223621) 1  I knew that I wanted to read it again because it was so good and I was missing the subleties\n",
            "((782, 1), 9.651988) 1  Haven't finished the book, but love it\n",
            "prototype index =  5\n",
            "((1785, 2), 0.0) 0  Not durable and not recommended\n",
            "((569, 0), 4.254093) 0 Not recommended\n",
            "((1019, 5), 4.254093) 0 )Not recommended\n",
            "((2246, 6), 4.254093) 0  Not recommended\n",
            "((2522, 0), 4.254093) 0 Not recommended\n",
            "prototype index =  6\n",
            "((780, 4), 0.0) 0  That is all fine and good if the book is worth the effort, but this one definitely is not\n",
            "((524, 5), 7.087709) 1  In summary, well worth your time to read but not a good enough book to truly fit it's title\n",
            "((821, 2), 7.6546774) 0  Sure the main idea and a well written summary makes the book as a whole good, it doesn't help the actual context one bit which was very hard to follow and comprehend\n",
            "((2016, 2), 8.073212) 1  This is a very good book, whose only shortcoming is a lack of information\n",
            "((754, 0), 8.25458) 1 Decent book, but definately not Hanry David Theraou\n",
            "prototype index =  7\n",
            "((1736, 4), 0.0) 1  the good part is that even though it holds great amounts of information, the book is orgenized in a way that is easily referenced in the future\n",
            "((796, 5), 9.204886) 0  While this book has a rather good story it`s concealed by endless symbols and references, I suggest the cliff notes version\n",
            "((1746, 5), 9.80293) 1  Some of the book will undoubtedly be dated, but much of it is just as relevant -\n",
            "((869, 4), 9.848344) 1  i think this is overly harsh, yet this whole book reflects the time in which it took place very well\n",
            "((2222, 2), 10.103968) 0  It is an interesting book although it touches on everything and seems to repeat itself time after time\n",
            "prototype index =  8\n",
            "((1797, 8), 0.0) 1 sortaweb\n",
            "((609, 0), 8.079719) 1 Awsome\n",
            "((354, 7), 8.331896) 0  Eeeeew\n",
            "((218, 5), 8.405623) 1  Thats boloney\n",
            "((2492, 0), 8.530228) 0 's reveiw\n",
            "prototype index =  9\n",
            "((1339, 5), 0.0) 0  Brown and John Grover have the potential to write something good, but it seems to be, at the moment at least, unfulfilled potential\n",
            "((2572, 2), 9.023763) 0  I feel there is alot of good information but not worth buying\n",
            "((1889, 6), 9.2459755) 0  I'd love to find a good one of these but this one is not\n",
            "((1564, 5), 9.832008) 0 It seemed like a great idea, but the products is not well made\n",
            "((1626, 6), 10.017777) 0  This song is worth getting, but not for $$$ \n",
            "prototype index =  10\n",
            "((796, 5), 0.0) 0  While this book has a rather good story it`s concealed by endless symbols and references, I suggest the cliff notes version\n",
            "((1736, 4), 9.204886) 1  the good part is that even though it holds great amounts of information, the book is orgenized in a way that is easily referenced in the future\n",
            "((743, 2), 9.630603) 0  Although it was a very interesting and provkitive novel\n",
            "((2668, 4), 9.654077) 1  Suprisingly, though all discouraging factors were present, I found the book exceptionally appealing\n",
            "((2248, 3), 9.658695) 1 \"Despite its rather disorganized and somewhat repetitive presentation (again, fairly standard for this genre), Keel relates his narrative earnestly, and he will engage your interest\n",
            "prototype index =  11\n",
            "((824, 2), 0.0) 0  Although it has received over 500 reviews I feel it is my duty to help bring the average star level to where it belongs, at zero\n",
            "((1676, 2), 9.460556) 0  I think I will have to agree with the previous reviewer that gave a low rating\n",
            "((2460, 6), 10.649873) 0  So basically, if you must have the few extras, find it cheap or used, and if you're a first timer, let this be the last thing you add to your collection\n",
            "((1005, 4), 10.692299) 0  They did one for the Hero Of The Day single, limiting it to 1,000 copies\n",
            "((1293, 5), 10.880701) 1  I knocked off one star because they run a bit small; order a size up if in doubt\n",
            "prototype index =  12\n",
            "((1649, 4), 0.0) 1  Like the book states, Not all parts will pertain to you at this time, but it is a wonderful refference for later\n",
            "((828, 8), 9.331765) 1  The book may not be as exciting as some, but don't let this keep you from reading it, because it really is good\n",
            "((1796, 2), 9.854936) 1  The 4000 words utilized are good, but they won't take you to the next level\n",
            "((2629, 3), 9.942517) 0  They may have corrected the problem in other copies, but don't assume that you'll get the whole book\n",
            "((759, 5), 10.07969) 0  It did catch my interest a little in the end, but not enough to redeem the book in any way\n",
            "prototype index =  13\n",
            "((610, 2), 0.0) 0  That worked well, but i wonder why i dint buy the same and ended up with this\n",
            "((21, 1), 9.048139) 1  I thought it was funny that I bought this product without knowing it was a mix\n",
            "((618, 7), 9.648743) 1  (Not exactly why I bought it but o well)\n",
            "((1034, 11), 9.884566) 0  but if you knew how good they were on leifmotif, don't buy this\n",
            "((661, 1), 9.894325) 0  This product was not what I expected so I returned it\n",
            "prototype index =  14\n",
            "((1564, 5), 0.0) 0 It seemed like a great idea, but the products is not well made\n",
            "((1155, 2), 7.336607) 0  It looks like a good product, but the plug doesn't fit\n",
            "((341, 2), 8.286717) 0  This is a great book, but it was not bound well\n",
            "((1484, 7), 8.520417) 0  Gameplay is tolerable, but nowhere near as good as I had hoped it would be\n",
            "((2857, 5), 8.728727) 0  The special effects are still amazing, but there just wasn't enough action\n",
            "prototype index =  15\n",
            "((1131, 3), 0.0) 1  It works great\n",
            "((1519, 4), 0.0) 1  It works great\n",
            "((649, 1), 3.3103924) 1  This works really well\n",
            "((1133, 2), 3.37877) 1  but works great\n",
            "((665, 1), 3.4919314) 1  It really works\n",
            "prototype index =  16\n",
            "((1034, 11), 0.0) 0  but if you knew how good they were on leifmotif, don't buy this\n",
            "((627, 11), 9.472122) 0  For a product thats is suppose to make you feel good about yourself, it failed\n",
            "((2330, 5), 9.532258) 1  I just wonder if they really know just how good they were and how much they are missed\n",
            "((2512, 0), 9.576647) 1 good lord - i cant see how it gets any better than this\n",
            "((611, 4), 9.601933) 1  I just cannot express in words how great this product is\n",
            "prototype index =  17\n",
            "((1005, 4), 0.0) 0  They did one for the Hero Of The Day single, limiting it to 1,000 copies\n",
            "((924, 2), 10.460098) 0  Frankly, a 1996 edition should be sold as remainders at about $2, then someone who wanted only info about the people or the temples could get ethnographic info only\n",
            "((2291, 3), 10.479181) 0  Once to corrected shipment arrived again it was a service for only 1\n",
            "((2460, 6), 10.656145) 0  So basically, if you must have the few extras, find it cheap or used, and if you're a first timer, let this be the last thing you add to your collection\n",
            "((824, 2), 10.692299) 0  Although it has received over 500 reviews I feel it is my duty to help bring the average star level to where it belongs, at zero\n",
            "prototype index =  18\n",
            "((1796, 2), 0.0) 1  The 4000 words utilized are good, but they won't take you to the next level\n",
            "((189, 4), 8.965435) 0 MASK MAKER isn't the worst entry in this familiar territory but it doesn't tread new waters either\n",
            "((1325, 11), 9.089376) 0 It's a cute product if you are ok with something that is designed to not be used beyond 2-3 refills\n",
            "((1021, 3), 9.124738) 0  It is very durable, but we can't get rid of the smell or stains with a reasonable amount of effort\n",
            "((835, 8), 9.228987) 0  Now I know there wasn't much around to inspire good stories back than but I didn't think it was this bad\n",
            "prototype index =  19\n",
            "((1063, 5), 0.0) 1  This is by no means a perfect cd - but my rage at the other review has pushed it from a 4 to a 5\n",
            "((1295, 9), 8.949821) 0  THE BAND ITSELF RATES BETTER THAN 2 STARS, BUT NOT HAVING THE SONGS IT SHOULD OF HAD,GREATLY DEPRECIATES THIS CD\n",
            "((2295, 1), 9.611731) 1  The CD will not grab you at first, its not as catchy as Lagwagon's previous releases, but with a few listens it makes the argument for greatest Lag album ever\n",
            "((2475, 1), 9.744553) 0  ok--first of all, saying that this CD is the \"best\" of anything is completely wrong, and don't give me that Bullsh*t about \"you try to write something\" thats not the issue\n",
            "((183, 7), 9.773254) 0 I blame my self for not being more curious of the number of CD's\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljNABGeCGuK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}